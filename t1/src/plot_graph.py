#!/usr/bin/env python3
"""
Script to generate analysis graphs of benchmark results for data structures.
Analyzes cache, time, and energy metrics for different datasets, structures, and operations.
"""

import os
import glob
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# Settings
RESULTS_DIR = "../results/graph"
PLOTS_DIR = "../plots/graph"
STRUCTURES = ["matrix", "list", "hash", "matrix_vec", "list_vec", "hash_umap"]
OPERATIONS = ["insertion", "components", "clustering"]
METRICS = ["cache", "time", "energy", "edp"]

# Consistent colors per structure
COLOR_MAP = {
    'matrix': '#1f77b4',        # blue
    'matrix_vec': '#6baed6',    # light blue
    'list': '#2ca02c',          # green
    'list_vec': '#98df8a',      # light green
    'hash': '#d62728',          # red
    'hash_umap': '#ff9896',     # light red
}

# More descriptive labels for plots
METRIC_LABELS = {
    "cache": "Cache Misses",
    "time": "Execution Time (s)",
    "energy": "Energy (J)",
    "edp": "EDP (J·s)"
}

STRUCTURE_LABELS = {
    "matrix": "Matrix (calloc)",
    "list": "List (calloc)",
    "hash": "Hash (open-addressing)",
    "matrix_vec": "Matrix (std::vector)",
    "list_vec": "List (std::vector)",
    "hash_umap": "Hash (unordered_map)"
}

OPERATION_LABELS = {
    "insertion": "Insertion",
    "components": "Components",
    "clustering": "Clustering"
}

# Style configuration
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['font.size'] = 10


def parse_cache_csv(filepath):
    """
    Parses cache output file generated by perf stat.
    Extracts the first numeric column from lines containing 'L1-dcache-load-misses'.
    Removes thousands separators (commas).
    """
    values = []
    try:
        with open(filepath, 'r') as f:
            for line in f:
                if 'L1-dcache-load-misses' not in line:
                    continue
                # Example line: '   2,461,234      L1-dcache-load-misses    # ... ( +- 0.50% )'
                parts = line.strip().split()
                if not parts:
                    continue
                num = parts[0].replace(',', '')
                try:
                    values.append(float(num))
                except ValueError:
                    continue
    except FileNotFoundError:
        return []
    return values


def parse_simple_csv(filepath):
    """
    Parses simple output files (one numeric value per line) and removes thousands separators.
    Falls back to extracting the last token if the line has multiple tokens.
    """
    values = []
    try:
        with open(filepath, 'r') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                # Prefer last whitespace-separated token, then remove commas
                token = line.split()[-1].replace(',', '')
                try:
                    values.append(float(token))
                except ValueError:
                    # As fallback, split by comma and try the last segment
                    try:
                        values.append(float(line.replace(',', '')))
                    except ValueError:
                        continue
    except FileNotFoundError:
        return []
    return values


def load_metric_data(dataset, structure, operation, metric):
    """
    Loads data for a specific metric and returns mean and stddev.
    For EDP, calculates the product of energy and time.
    """
    if metric == "edp":
        # EDP = Energy × Time
        energy_mean, energy_std = load_metric_data(dataset, structure, operation, "energy")
        time_mean, time_std = load_metric_data(dataset, structure, operation, "time")
        if energy_mean is None or time_mean is None:
            return None, None
        edp_mean = energy_mean * time_mean
        if energy_std is not None and time_std is not None and energy_mean and time_mean:
            relative_error = np.sqrt((energy_std/energy_mean)**2 + (time_std/time_mean)**2)
            edp_std = edp_mean * relative_error
        else:
            edp_std = 0
        return edp_mean, edp_std

    # Use .out files generated by run_graph.sh
    filename = f"{dataset}_{structure}_{operation}_{metric}.out"
    filepath = os.path.join(RESULTS_DIR, filename)

    if not os.path.exists(filepath):
        return None, None

    if metric == "cache":
        values = parse_cache_csv(filepath)
    else:
        values = parse_simple_csv(filepath)

    if len(values) == 0:
        return None, None

    return np.mean(values), np.std(values)


def get_available_datasets():
    """
    Identifies all available datasets in the results by scanning *.out files.
    """
    datasets = set()
    for filepath in glob.glob(os.path.join(RESULTS_DIR, "*_*.out")):
        filename = os.path.basename(filepath).replace(".out", "")
        parts = filename.split("_")
        # dataset is everything before the structure token
        dataset_parts = []
        for part in parts:
            if part in STRUCTURES:
                break
            dataset_parts.append(part)
        if dataset_parts:
            datasets.add("_".join(dataset_parts))
    return sorted(list(datasets))


def plot_dataset_comparison(dataset):
    """
    Generates plots comparing all data structures for a specific dataset.
    Creates a subplot for each operation and metric combination.
    """
    print(f"Generating comparison plots for dataset: {dataset}")
    
    fig, axes = plt.subplots(3, 4, figsize=(20, 14))
    fig.suptitle(f'Data Structure Comparison - Dataset: {dataset}', 
                 fontsize=16, fontweight='bold')
    
    for op_idx, operation in enumerate(OPERATIONS):
        for met_idx, metric in enumerate(METRICS):
            ax = axes[op_idx, met_idx]
            
            means = []
            stds = []
            labels = []
            
            colors = []
            for structure in STRUCTURES:
                mean, std = load_metric_data(dataset, structure, operation, metric)
                if mean is not None:
                    means.append(mean)
                    stds.append(std)
                    labels.append(STRUCTURE_LABELS[structure])
                    colors.append(COLOR_MAP.get(structure, '#7f7f7f'))
            
            if means:
                x_pos = np.arange(len(labels))
                bars = ax.bar(x_pos, means, yerr=stds, capsize=5, alpha=0.7, color=colors)
                ax.set_xticks(x_pos)
                ax.set_xticklabels(labels, rotation=45, ha='right')
                ax.set_ylabel(METRIC_LABELS[metric])
                ax.set_title(f'{OPERATION_LABELS[operation]} - {METRIC_LABELS[metric]}')
                ax.grid(True, alpha=0.3)
                
                # Add values above bars
                for i, (bar, mean_val) in enumerate(zip(bars, means)):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height,
                           f'{mean_val:.2e}' if mean_val > 1000 else f'{mean_val:.2f}',
                           ha='center', va='bottom', fontsize=8)
            else:
                ax.text(0.5, 0.5, 'Data not available', 
                       ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'{OPERATION_LABELS[operation]} - {METRIC_LABELS[metric]}')
    
    plt.tight_layout()
    output_file = os.path.join(PLOTS_DIR, f'{dataset}_comparison.png')
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"  Saved: {output_file}")
    # Log-scale variant
    for ax in np.ravel(axes):
        ax.set_yscale('log')
    output_file_log = os.path.join(PLOTS_DIR, f'{dataset}_comparison_log.png')
    plt.savefig(output_file_log, dpi=300, bbox_inches='tight')
    print(f"  Saved: {output_file_log}")
    plt.close()

def plot_naive_vs_optimized(dataset):
    """
    Compares naive vs optimized implementations (Matrix, List, Hash) for each
    operation and metric in a grid of subplots.
    Each subplot: groups are categories (Matrix/List/Hash); each group has two bars: Optimized vs Naive.
    """
    print(f"Generating naive vs optimized comparison for dataset: {dataset}")

    categories = [
        ("Matrix", "matrix", "matrix_vec"),
        ("List", "list", "list_vec"),
        ("Hash", "hash", "hash_umap"),
    ]

    fig, axes = plt.subplots(3, 4, figsize=(20, 14))
    fig.suptitle(f'Naive vs Optimized - Dataset: {dataset}',
                 fontsize=16, fontweight='bold')

    for op_idx, operation in enumerate(OPERATIONS):
        for met_idx, metric in enumerate(METRICS):
            ax = axes[op_idx, met_idx]

            group_labels = []
            # Normalized values: naive = 1.0, optimized = optimized/naive (lower is better)
            nai_norm_means, nai_norm_stds = [], []
            opt_norm_means, opt_norm_stds = [], []

            for label, opt_key, nai_key in categories:
                # Include Matrix in insertion to compare naive vs optimized

                opt_mean, opt_std = load_metric_data(dataset, opt_key, operation, metric)
                nai_mean, nai_std = load_metric_data(dataset, nai_key, operation, metric)

                # Só plota grupos com dados disponíveis para ambas as variantes
                if opt_mean is not None and nai_mean is not None and nai_mean > 0:
                    group_labels.append(label)
                    # naive normalized to 1, std set to 0 (baseline)
                    nai_norm_means.append(1.0)
                    nai_norm_stds.append(0.0)
                    # optimized normalized as ratio = optimized/naive (lower is better)
                    ratio = opt_mean / nai_mean
                    # propagate relative error if available: sigma(r)/r = sqrt((sigma_opt/opt)^2 + (sigma_nai/nai)^2)
                    if (opt_std is not None and opt_mean) and (nai_std is not None and nai_mean):
                        rel_o = (opt_std / opt_mean) if opt_mean != 0 else 0.0
                        rel_n = (nai_std / nai_mean) if nai_mean != 0 else 0.0
                        ratio_std = ratio * np.sqrt(rel_o**2 + rel_n**2)
                    else:
                        ratio_std = 0.0
                    opt_norm_means.append(ratio)
                    opt_norm_stds.append(ratio_std)

            if group_labels:
                x = np.arange(len(group_labels))
                width = 0.38
                # Plot naive (baseline = 1) and optimized (speedup)
                bars_nai = ax.bar(x - width/2, nai_norm_means, width, yerr=nai_norm_stds, label='Naive (baseline=1)',
                                  capsize=5, alpha=0.8, color='#ff7f0e')
                bars_opt = ax.bar(x + width/2, opt_norm_means, width, yerr=opt_norm_stds, label='Optimized (speedup)',
                                  capsize=5, alpha=0.8, color='#1f77b4')

                ax.set_xticks(x)
                ax.set_xticklabels(group_labels, rotation=0, ha='center')
                ax.set_ylabel(f"Normalized {METRIC_LABELS[metric]} (naive = 1; lower is better)")
                ax.set_title(f'{OPERATION_LABELS[operation]} - {METRIC_LABELS[metric]}')
                ax.grid(True, alpha=0.3, axis='y')
                ax.legend(fontsize=9)
                # baseline at 1
                ax.axhline(1.0, color='#7f7f7f', linewidth=1, linestyle='--')

                # Values above bars
                for bar, mean_val in zip(bars_nai, nai_norm_means):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height,
                            f'{mean_val:.2f}x',
                            ha='center', va='bottom', fontsize=8)
                for bar, mean_val in zip(bars_opt, opt_norm_means):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height,
                            f'{mean_val:.2f}x',
                            ha='center', va='bottom', fontsize=8)
            else:
                ax.text(0.5, 0.5, 'Data not available',
                        ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f'{OPERATION_LABELS[operation]} - {METRIC_LABELS[metric]}')

    plt.tight_layout()
    output_file = os.path.join(PLOTS_DIR, f'{dataset}_naive_vs_optimized.png')
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"  Saved: {output_file}")
    # Log-scale variant
    for ax in np.ravel(axes):
        ax.set_yscale('log')
    output_file_log = os.path.join(PLOTS_DIR, f'{dataset}_naive_vs_optimized_log.png')
    plt.savefig(output_file_log, dpi=300, bbox_inches='tight')
    print(f"  Saved: {output_file_log}")
    plt.close()

def plot_heatmaps(dataset):
    """
    Generates heatmaps to visualize the relative performance of each structure
    in different operations for each metric.
    """
    print(f"Generating heatmaps for dataset: {dataset}")
    
    fig, axes = plt.subplots(1, 4, figsize=(24, 5))
    fig.suptitle(f'Performance Heatmap - Dataset: {dataset}',
                fontsize=16, fontweight='bold')
    
    for met_idx, metric in enumerate(METRICS):
        ax = axes[met_idx]
        
        # Cria matriz de dados
        data_matrix = np.zeros((len(STRUCTURES), len(OPERATIONS)))
        
        for struct_idx, structure in enumerate(STRUCTURES):
            for op_idx, operation in enumerate(OPERATIONS):
                # Skip matrix for insertion operation
                if operation == "insertion" and structure == "matrix":
                    data_matrix[struct_idx, op_idx] = np.nan
                    continue
                
                mean, _ = load_metric_data(dataset, structure, operation, metric)
                if mean is not None:
                    data_matrix[struct_idx, op_idx] = mean
                else:
                    data_matrix[struct_idx, op_idx] = np.nan
        
        # Cria heatmap
        im = ax.imshow(data_matrix, cmap='YlOrRd', aspect='auto')
        
        # Configurações dos eixos
        ax.set_xticks(np.arange(len(OPERATIONS)))
        ax.set_yticks(np.arange(len(STRUCTURES)))
        ax.set_xticklabels([OPERATION_LABELS[op] for op in OPERATIONS])
        ax.set_yticklabels([STRUCTURE_LABELS[st] for st in STRUCTURES])
        ax.set_title(METRIC_LABELS[metric])
        
        # Adiciona valores nas células
        for i in range(len(STRUCTURES)):
            for j in range(len(OPERATIONS)):
                value = data_matrix[i, j]
                if not np.isnan(value):
                    text = ax.text(j, i, f'{value:.2e}' if value > 1000 else f'{value:.2f}',
                                 ha="center", va="center", color="black", fontsize=9)
        
        # Adiciona colorbar
        plt.colorbar(im, ax=ax)
    
    plt.tight_layout()
    output_file = os.path.join(PLOTS_DIR, f'{dataset}_heatmap.png')
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"  Saved: {output_file}")
    plt.close()

def plot_edp_analysis(dataset):
    """
    Generates a specific plot for EDP (Energy Delay Product) analysis.
    Shows the energy efficiency of each structure in different operations.
    """
    print(f"Generating EDP analysis for dataset: {dataset}")
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    fig.suptitle(f'EDP Analysis (Energy Delay Product) - Dataset: {dataset}',
                fontsize=16, fontweight='bold')
    
    for op_idx, operation in enumerate(OPERATIONS):
        ax = axes[op_idx]
        
        means = []
        stds = []
        labels = []
        colors = []
        color_map = {
            'matrix': '#1f77b4',
            'matrix_vec': '#6baed6',
            'list': '#2ca02c',
            'list_vec': '#98df8a',
            'hash': '#d62728',
            'hash_umap': '#ff9896',
        }
        
        for structure in STRUCTURES:
            # Skip matrix for insertion operation
            if operation == "insertion" and structure == "matrix":
                continue
            
            mean, std = load_metric_data(dataset, structure, operation, "edp")
            if mean is not None:
                means.append(mean)
                stds.append(std)
                labels.append(STRUCTURE_LABELS[structure])
                colors.append(color_map[structure])
        
        if means:
            x_pos = np.arange(len(labels))
            bars = ax.bar(x_pos, means, yerr=stds, capsize=5, alpha=0.7, color=colors)
            ax.set_xticks(x_pos)
            ax.set_xticklabels(labels, rotation=45, ha='right')
            ax.set_ylabel('EDP (J·s)')
            ax.set_title(f'{OPERATION_LABELS[operation]}')
            ax.grid(True, alpha=0.3, axis='y')
            
            # Add values above bars
            for bar, mean_val in zip(bars, means):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{mean_val:.2e}' if mean_val > 1000 else f'{mean_val:.2f}',
                       ha='center', va='bottom', fontsize=9)
            
            # Highlight the best option (lowest EDP)
            if means:
                min_idx = means.index(min(means))
                bars[min_idx].set_edgecolor('green')
                bars[min_idx].set_linewidth(3)
        else:
            ax.text(0.5, 0.5, 'Data not available',
                   ha='center', va='center', transform=ax.transAxes)
            ax.set_title(f'{OPERATION_LABELS[operation]}')
    
    plt.tight_layout()
    output_file = os.path.join(PLOTS_DIR, f'{dataset}_edp_analysis.png')
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"  Saved: {output_file}")
    plt.close()

def generate_summary_table(dataset):
    """
    Generates a summary table in CSV with all metrics.
    """
    print(f"Generating summary table for dataset: {dataset}")
    
    rows = []
    for structure in STRUCTURES:
        for operation in OPERATIONS:
            # Skip matrix for insertion operation
            if operation == "insertion" and structure == "matrix":
                continue
            
            row = {
                'dataset': dataset,
                'structure': structure,
                'operation': operation
            }
            
            for metric in METRICS:
                mean, std = load_metric_data(dataset, structure, operation, metric)
                row[f'{metric}_mean'] = mean if mean is not None else np.nan
                row[f'{metric}_std'] = std if std is not None else np.nan
            
            rows.append(row)
    
    df = pd.DataFrame(rows)
    output_file = os.path.join(PLOTS_DIR, f'{dataset}_summary.csv')
    df.to_csv(output_file, index=False)
    print(f"  Saved: {output_file}")

def main():
    """
    Main function that orchestrates the generation of all plots.
    """
    # Create plots directory if it does not exist
    os.makedirs(PLOTS_DIR, exist_ok=True)
    
    print("="*60)
    print("Starting results analysis")
    print("="*60)
    
    # Identify available datasets
    datasets = get_available_datasets()
    print(f"\nDatasets found: {datasets}")
    print(f"Structures: {STRUCTURES}")
    print(f"Operations: {OPERATIONS}")
    print(f"Metrics: {METRICS}")
    print()
    
    # Generate plots for each dataset
    for dataset in datasets:
        print(f"\n{'='*60}")
        print(f"Processing dataset: {dataset}")
        print(f"{'='*60}")
        plot_dataset_comparison(dataset)
        plot_edp_analysis(dataset)
        generate_summary_table(dataset)
    
    # Generate naive vs optimized comparison plots
    print(f"\n{'='*60}")
    print("Naive vs Optimized comparisons")
    print(f"{'='*60}")
    for dataset in datasets:
        plot_naive_vs_optimized(dataset)
    
    print(f"\n{'='*60}")
    print("Analysis finished!")
    print(f"Plots saved in: {PLOTS_DIR}/")
    print(f"{'='*60}")

if __name__ == "__main__":
    main()
